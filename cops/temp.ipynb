{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T22:27:50.591667Z",
     "start_time": "2025-05-18T22:27:30.094857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Add comments before import\n",
    "# Gradient descent with enhanced features, regularization, and standardization\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and parse data\n",
    "with open('updated_housing.csv', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i, line in enumerate(lines[1:], start=2):\n",
    "    parts = line.strip().split(',')\n",
    "    if len(parts) < 10:\n",
    "        continue\n",
    "    try:\n",
    "        longitude = float(parts[0])\n",
    "        latitude = float(parts[1])\n",
    "        age = float(parts[2])\n",
    "        total_rooms = float(parts[3])\n",
    "        total_bedrooms = float(parts[4])\n",
    "        population = float(parts[5])\n",
    "        households = float(parts[6])\n",
    "        income = min(float(parts[7]), 15)  # clip high income\n",
    "        value = float(parts[8])\n",
    "\n",
    "        if total_rooms == 0 or households == 0:\n",
    "            continue\n",
    "\n",
    "        rooms_per_person = total_rooms / population if population != 0 else 0\n",
    "        bedrooms_per_room = total_bedrooms / total_rooms\n",
    "        income_per_household = income / households\n",
    "        rooms_per_household = total_rooms / households\n",
    "        population_per_household = population / households\n",
    "\n",
    "        features = [\n",
    "            longitude,\n",
    "            latitude,\n",
    "            age,\n",
    "            income,\n",
    "            rooms_per_person,\n",
    "            bedrooms_per_room,\n",
    "            rooms_per_household,\n",
    "            population_per_household,\n",
    "            income_per_household\n",
    "        ]\n",
    "\n",
    "        x_data.append(features)\n",
    "        y_data.append(value)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "# Step 2: Clip target outliers\n",
    "y_data = np.clip(y_data, a_min=None, a_max=np.percentile(y_data, 99))\n",
    "\n",
    "# Step 3: Standardize features\n",
    "x_mean = x_data.mean(axis=0)\n",
    "x_std = x_data.std(axis=0)\n",
    "x_data = (x_data - x_mean) / x_std\n",
    "x_data = np.nan_to_num(x_data)\n",
    "\n",
    "# Add bias term\n",
    "x_data = np.hstack([np.ones((x_data.shape[0], 1)), x_data])\n",
    "\n",
    "# Normalize target (min-max)\n",
    "y_min = y_data.min()\n",
    "y_max = y_data.max()\n",
    "y_data = (y_data - y_min) / (y_max - y_min)\n",
    "y_data = y_data.reshape(-1, 1)\n",
    "\n",
    "# Step 4: Train-test split\n",
    "split = int(0.8 * len(x_data))\n",
    "x_train, x_test = x_data[:split], x_data[split:]\n",
    "y_train, y_test = y_data[:split], y_data[split:]\n",
    "\n",
    "# Step 5: Gradient Descent with L2 regularization\n",
    "weights = np.random.randn(x_train.shape[1], 1) * 0.01\n",
    "alpha = 0.001\n",
    "epochs = 20000\n",
    "lambda_reg = 0.1\n",
    "n = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    predictions = x_train @ weights\n",
    "    errors = predictions - y_train\n",
    "    gradient = (x_train.T @ errors) / n + lambda_reg * weights\n",
    "    gradient[0] -= lambda_reg * weights[0]  # don't regularize bias\n",
    "    weights -= alpha * gradient\n",
    "\n",
    "# Step 6: Evaluation\n",
    "def evaluate(x, y_true, weights):\n",
    "    preds = x @ weights\n",
    "    errors = y_true - preds\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    rmse = np.sqrt(np.mean(errors**2))\n",
    "    y_mean = np.mean(y_true)\n",
    "    total_var = np.sum((y_true - y_mean)**2)\n",
    "    residual_var = np.sum(errors**2)\n",
    "    r2 = 1 - residual_var / total_var\n",
    "    return mae, rmse, r2\n",
    "\n",
    "mae, rmse, r2 = evaluate(x_test, y_test, weights)\n",
    "\n",
    "# Step 7: Predict custom input\n",
    "custom_input = np.array([\n",
    "    -122.23,     # longitude\n",
    "    37.88,       # latitude\n",
    "    41,          # housing_median_age\n",
    "    8.3252,      # median_income\n",
    "    880 / 322,   # rooms_per_person\n",
    "    129 / 880,   # bedrooms_per_room\n",
    "    880 / 126,   # rooms_per_household\n",
    "    322 / 126,   # population_per_household\n",
    "    8.3252 / 126 # income_per_household\n",
    "])\n",
    "\n",
    "custom_input = (custom_input - x_mean) / x_std\n",
    "custom_input = np.nan_to_num(custom_input)\n",
    "custom_input = np.insert(custom_input, 0, 1.0)\n",
    "predicted_normalized = custom_input @ weights\n",
    "predicted_normalized = np.clip(predicted_normalized, 0, 1)\n",
    "predicted_value = predicted_normalized * (y_max - y_min) + y_min\n",
    "\n",
    "# Step 8: Output\n",
    "print(\"Trained weights:\", weights.flatten())\n",
    "print(\"Predicted median_house_value:\", predicted_value.item())\n",
    "print(\"MAE:\", mae * (y_max - y_min))\n",
    "print(\"RMSE:\", rmse * (y_max - y_min))\n",
    "print(\"R² Score:\", r2)"
   ],
   "id": "eb1e71707856c78f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [ 0.38798288 -0.05588284 -0.07095369  0.03397746  0.16080368  0.06126588\n",
      "  0.02889495 -0.04479205 -0.00894681 -0.00349121]\n",
      "Predicted median_house_value: 391313.3049810674\n",
      "MAE: 53550.71750967181\n",
      "RMSE: 73574.56164320438\n",
      "R² Score: 0.628508772566651\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:23:48.893621Z",
     "start_time": "2025-05-18T21:23:34.291196Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "e9d8d76ac24ecd79",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [ 0.38815284 -0.05900047 -0.0741089   0.03359259  0.1602105   0.06207382\n",
      "  0.02843193 -0.04538673 -0.00861879 -0.00343335]\n",
      "Predicted median_house_value: 391274.4483565571\n",
      "MAE: 53279.0088844788\n",
      "RMSE: 73242.57419108084\n",
      "R² Score: 0.6318537378356985\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T21:29:42.333620Z",
     "start_time": "2025-05-18T21:29:42.323989Z"
    }
   },
   "cell_type": "code",
   "source": "0.6318537378356985",
   "id": "cf3fac25536eb37f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6318537378356985"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T11:56:16.235378Z",
     "start_time": "2025-05-19T11:55:12.983375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradient descent with enhanced features, regularization, standardization, and ocean_proximity one-hot encoding\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load and parse data\n",
    "with open('updated_housing.csv', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "# Collect unique ocean_proximity categories (strings of numbers)\n",
    "categories = set()\n",
    "for line in lines[1:]:\n",
    "    parts = line.strip().split(',')\n",
    "    if len(parts) < 10:\n",
    "        continue\n",
    "    categories.add(parts[9])\n",
    "\n",
    "categories = sorted(categories)\n",
    "cat_to_index = {cat: i for i, cat in enumerate(categories)}\n",
    "\n",
    "for line in lines[1:]:\n",
    "    parts = line.strip().split(',')\n",
    "    if len(parts) < 10:\n",
    "        continue\n",
    "    try:\n",
    "        longitude = float(parts[0])\n",
    "        latitude = float(parts[1])\n",
    "        age = float(parts[2])\n",
    "        total_rooms = float(parts[3])\n",
    "        total_bedrooms = float(parts[4])\n",
    "        population = float(parts[5])\n",
    "        households = float(parts[6])\n",
    "        income = min(float(parts[7]), 15)  # clip high income\n",
    "        value = float(parts[8])\n",
    "        ocean_proximity = parts[9]\n",
    "\n",
    "        if total_rooms == 0 or households == 0:\n",
    "            continue\n",
    "\n",
    "        rooms_per_person = total_rooms / population if population != 0 else 0\n",
    "        bedrooms_per_room = total_bedrooms / total_rooms\n",
    "        income_per_household = income / households\n",
    "        rooms_per_household = total_rooms / households\n",
    "        population_per_household = population / households\n",
    "\n",
    "        features = [\n",
    "            longitude,\n",
    "            latitude,\n",
    "            age,\n",
    "            income,\n",
    "            rooms_per_person,\n",
    "            bedrooms_per_room,\n",
    "            rooms_per_household,\n",
    "            population_per_household,\n",
    "            income_per_household\n",
    "        ]\n",
    "\n",
    "        # One-hot encode ocean_proximity\n",
    "        one_hot = [0] * len(categories)\n",
    "        one_hot[cat_to_index[ocean_proximity]] = 1\n",
    "        features.extend(one_hot)\n",
    "\n",
    "        x_data.append(features)\n",
    "        y_data.append(value)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "# Step 2: Clip target outliers\n",
    "y_data = np.clip(y_data, a_min=None, a_max=np.percentile(y_data, 99))\n",
    "\n",
    "# Step 3: Standardize features (except one-hot encoded ocean_proximity columns)\n",
    "num_features = 9\n",
    "num_categories = len(categories)\n",
    "x_num = x_data[:, :num_features]\n",
    "x_cat = x_data[:, num_features:]\n",
    "\n",
    "x_mean = x_num.mean(axis=0)\n",
    "x_std = x_num.std(axis=0)\n",
    "x_num = (x_num - x_mean) / x_std\n",
    "x_num = np.nan_to_num(x_num)\n",
    "\n",
    "# Combine standardized numerical features and one-hot categorical features\n",
    "x_data = np.hstack([x_num, x_cat])\n",
    "\n",
    "# Add bias term\n",
    "x_data = np.hstack([np.ones((x_data.shape[0], 1)), x_data])\n",
    "\n",
    "# Normalize target (min-max)\n",
    "y_min = y_data.min()\n",
    "y_max = y_data.max()\n",
    "y_data = (y_data - y_min) / (y_max - y_min)\n",
    "y_data = y_data.reshape(-1, 1)\n",
    "\n",
    "# Step 4: Train-test split\n",
    "split = int(0.8 * len(x_data))\n",
    "x_train, x_test = x_data[:split], x_data[split:]\n",
    "y_train, y_test = y_data[:split], y_data[split:]\n",
    "\n",
    "# Step 5: Gradient Descent with L2 regularization\n",
    "weights = np.random.randn(x_train.shape[1], 1) * 0.01\n",
    "alpha = 0.001\n",
    "epochs = 20000\n",
    "lambda_reg = 0.1\n",
    "n = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    predictions = x_train @ weights\n",
    "    errors = predictions - y_train\n",
    "    gradient = (x_train.T @ errors) / n + lambda_reg * weights\n",
    "    gradient[0] -= lambda_reg * weights[0]  # don't regularize bias\n",
    "    weights -= alpha * gradient\n",
    "\n",
    "# Step 6: Evaluation\n",
    "def evaluate(x, y_true, weights):\n",
    "    preds = x @ weights\n",
    "    errors = y_true - preds\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    rmse = np.sqrt(np.mean(errors**2))\n",
    "    y_mean = np.mean(y_true)\n",
    "    total_var = np.sum((y_true - y_mean)**2)\n",
    "    residual_var = np.sum(errors**2)\n",
    "    r2 = 1 - residual_var / total_var\n",
    "    return mae, rmse, r2\n",
    "\n",
    "mae, rmse, r2 = evaluate(x_test, y_test, weights)\n",
    "\n",
    "# Step 7: Predict custom input\n",
    "# Custom input must have 9 numerical features + one-hot ocean_proximity vector\n",
    "custom_input_num = np.array([\n",
    "    -122.23,     # longitude\n",
    "    37.88,       # latitude\n",
    "    41,          # housing_median_age\n",
    "    8.3252,      # median_income\n",
    "    880 / 322,   # rooms_per_person\n",
    "    129 / 880,   # bedrooms_per_room\n",
    "    880 / 126,   # rooms_per_household\n",
    "    322 / 126,   # population_per_household\n",
    "    8.3252 / 126 # income_per_household\n",
    "])\n",
    "\n",
    "custom_input_num = (custom_input_num - x_mean) / x_std\n",
    "custom_input_num = np.nan_to_num(custom_input_num)\n",
    "\n",
    "# Example ocean_proximity value as string, e.g. '3'\n",
    "custom_ocean_proximity = '3'\n",
    "custom_one_hot = [0] * len(categories)\n",
    "if custom_ocean_proximity in cat_to_index:\n",
    "    custom_one_hot[cat_to_index[custom_ocean_proximity]] = 1\n",
    "\n",
    "custom_input = np.hstack([custom_input_num, custom_one_hot])\n",
    "custom_input = np.insert(custom_input, 0, 1.0)  # add bias\n",
    "\n",
    "predicted_normalized = custom_input @ weights\n",
    "predicted_normalized = np.clip(predicted_normalized, 0, 1)\n",
    "predicted_value = predicted_normalized * (y_max - y_min) + y_min\n",
    "\n",
    "# Step 8: Output\n",
    "print(\"Trained weights:\", weights.flatten())\n",
    "print(\"Predicted median_house_value:\", predicted_value.item())\n",
    "print(\"MAE:\", mae * (y_max - y_min))\n",
    "print(\"RMSE:\", rmse * (y_max - y_min))\n",
    "print(\"R² Score:\", r2)"
   ],
   "id": "e5b944d3f252dbf9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [ 0.37458268 -0.03583697 -0.04272122  0.0260748   0.14837575  0.0608504\n",
      "  0.02041791 -0.04154916 -0.00828297 -0.00274418  0.04383553 -0.04736629\n",
      "  0.00038598  0.04131973  0.02975843]\n",
      "Predicted median_house_value: 374143.0011870252\n",
      "MAE: 50036.146528997655\n",
      "RMSE: 70257.7502138597\n",
      "R² Score: 0.6612481516373423\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T11:41:17.849848Z",
     "start_time": "2025-05-19T11:41:17.840854Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "24b59900f732114",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T11:58:56.748291Z",
     "start_time": "2025-05-19T11:58:11.560793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradient descent with enhanced features, regularization, and standardization\n",
    "# Using numeric ocean_proximity from updated_ocean.csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load housing data\n",
    "with open('updated_housing.csv', 'r') as f_housing:\n",
    "    housing_lines = f_housing.readlines()\n",
    "\n",
    "# Load ocean_proximity numeric data\n",
    "with open('updated_ocean.csv', 'r') as f_ocean:\n",
    "    ocean_lines = f_ocean.readlines()\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i, line in enumerate(housing_lines[1:], start=1):  # align indexes with ocean file (skip header)\n",
    "    parts = line.strip().split(',')\n",
    "    ocean_parts = ocean_lines[i].strip().split(',')\n",
    "    if len(parts) < 9 or len(ocean_parts) < 10:\n",
    "        continue\n",
    "    try:\n",
    "        # Original features\n",
    "        longitude = float(parts[0])\n",
    "        latitude = float(parts[1])\n",
    "        age = float(parts[2])\n",
    "        total_rooms = float(parts[3])\n",
    "        total_bedrooms = float(parts[4])\n",
    "        population = float(parts[5])\n",
    "        households = float(parts[6])\n",
    "        income = min(float(parts[7]), 15)  # clip high income\n",
    "        value = float(parts[8])\n",
    "\n",
    "        # Skip invalid\n",
    "        if total_rooms == 0 or households == 0:\n",
    "            continue\n",
    "\n",
    "        # Derived features\n",
    "        rooms_per_person = total_rooms / population if population != 0 else 0\n",
    "        bedrooms_per_room = total_bedrooms / total_rooms\n",
    "        income_per_household = income / households\n",
    "        rooms_per_household = total_rooms / households\n",
    "        population_per_household = population / households\n",
    "\n",
    "        # Numeric ocean_proximity from 10th column of updated_ocean.csv\n",
    "        ocean_proximity = float(ocean_parts[9])\n",
    "\n",
    "        features = [\n",
    "            longitude,\n",
    "            latitude,\n",
    "            age,\n",
    "            income,\n",
    "            rooms_per_person,\n",
    "            bedrooms_per_room,\n",
    "            rooms_per_household,\n",
    "            population_per_household,\n",
    "            income_per_household,\n",
    "            ocean_proximity  # new 10th feature\n",
    "        ]\n",
    "\n",
    "        x_data.append(features)\n",
    "        y_data.append(value)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "# Clip target outliers\n",
    "y_data = np.clip(y_data, a_min=None, a_max=np.percentile(y_data, 99))\n",
    "\n",
    "# Standardize features\n",
    "x_mean = x_data.mean(axis=0)\n",
    "x_std = x_data.std(axis=0)\n",
    "x_data = (x_data - x_mean) / x_std\n",
    "x_data = np.nan_to_num(x_data)\n",
    "\n",
    "# Add bias term\n",
    "x_data = np.hstack([np.ones((x_data.shape[0], 1)), x_data])\n",
    "\n",
    "# Normalize target (min-max)\n",
    "y_min = y_data.min()\n",
    "y_max = y_data.max()\n",
    "y_data = (y_data - y_min) / (y_max - y_min)\n",
    "y_data = y_data.reshape(-1, 1)\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(x_data))\n",
    "x_train, x_test = x_data[:split], x_data[split:]\n",
    "y_train, y_test = y_data[:split], y_data[split:]\n",
    "\n",
    "# Gradient Descent with L2 regularization\n",
    "weights = np.random.randn(x_train.shape[1], 1) * 0.01\n",
    "alpha = 0.001\n",
    "epochs = 20000\n",
    "lambda_reg = 0.1\n",
    "n = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    predictions = x_train @ weights\n",
    "    errors = predictions - y_train\n",
    "    gradient = (x_train.T @ errors) / n + lambda_reg * weights\n",
    "    gradient[0] -= lambda_reg * weights[0]  # don't regularize bias\n",
    "    weights -= alpha * gradient\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(x, y_true, weights):\n",
    "    preds = x @ weights\n",
    "    errors = y_true - preds\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    rmse = np.sqrt(np.mean(errors**2))\n",
    "    y_mean = np.mean(y_true)\n",
    "    total_var = np.sum((y_true - y_mean)**2)\n",
    "    residual_var = np.sum(errors**2)\n",
    "    r2 = 1 - residual_var / total_var\n",
    "    return mae, rmse, r2\n",
    "\n",
    "mae, rmse, r2 = evaluate(x_test, y_test, weights)\n",
    "\n",
    "# Predict custom input (include ocean_proximity at last)\n",
    "custom_input = np.array([\n",
    "    -122.23,     # longitude\n",
    "    37.88,       # latitude\n",
    "    41,          # housing_median_age\n",
    "    8.3252,      # median_income\n",
    "    880 / 322,   # rooms_per_person\n",
    "    129 / 880,   # bedrooms_per_room\n",
    "    880 / 126,   # rooms_per_household\n",
    "    322 / 126,   # population_per_household\n",
    "    8.3252 / 126, # income_per_household\n",
    "    259212         # example ocean_proximity numeric (adjust accordingly)\n",
    "])\n",
    "\n",
    "custom_input = (custom_input - x_mean) / x_std\n",
    "custom_input = np.nan_to_num(custom_input)\n",
    "custom_input = np.insert(custom_input, 0, 1.0)\n",
    "predicted_normalized = custom_input @ weights\n",
    "predicted_normalized = np.clip(predicted_normalized, 0, 1)\n",
    "predicted_value = predicted_normalized * (y_max - y_min) + y_min\n",
    "\n",
    "print(\"Trained weights:\", weights.flatten())\n",
    "print(\"Predicted median_house_value:\", predicted_value.item())\n",
    "print(\"MAE:\", mae * (y_max - y_min))\n",
    "print(\"RMSE:\", rmse * (y_max - y_min))\n",
    "print(\"R² Score:\", r2)"
   ],
   "id": "4337ccd86ede6d7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [ 0.38908313 -0.02495756 -0.03160015  0.02297935  0.14504154  0.05840965\n",
      "  0.01804127 -0.03869517 -0.00800746 -0.00262673  0.05541898]\n",
      "Predicted median_house_value: 401006.07756051864\n",
      "MAE: 48581.512215039125\n",
      "RMSE: 68656.660837491\n",
      "R² Score: 0.6765117199229292\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:00:16.842054Z",
     "start_time": "2025-05-19T12:00:16.837279Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "55bf7b6955e0a3ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:07:16.462300Z",
     "start_time": "2025-05-19T12:06:56.398026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Gradient descent with enhanced features, regularization, and standardization\n",
    "# Loading all data from updated_ocean.csv only\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open('updated_ocean.csv', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i, line in enumerate(lines[1:], start=2):  # skip header\n",
    "    parts = line.strip().split(',')\n",
    "    if len(parts) < 10:\n",
    "        continue\n",
    "    try:\n",
    "        longitude = float(parts[0])\n",
    "        latitude = float(parts[1])\n",
    "        age = float(parts[2])\n",
    "        total_rooms = float(parts[3])\n",
    "        total_bedrooms = float(parts[4])\n",
    "        population = float(parts[5])\n",
    "        households = float(parts[6])\n",
    "        income = min(float(parts[7]), 15)  # clip high income\n",
    "        value = float(parts[8])\n",
    "        ocean_proximity = float(parts[9])  # numeric ocean proximity feature\n",
    "\n",
    "        if total_rooms == 0 or households == 0:\n",
    "            continue\n",
    "\n",
    "        rooms_per_person = total_rooms / population if population != 0 else 0\n",
    "        bedrooms_per_room = total_bedrooms / total_rooms\n",
    "        income_per_household = income / households\n",
    "        rooms_per_household = total_rooms / households\n",
    "        population_per_household = population / households\n",
    "\n",
    "        features = [\n",
    "            longitude,\n",
    "            latitude,\n",
    "            age,\n",
    "            income,\n",
    "            rooms_per_person,\n",
    "            bedrooms_per_room,\n",
    "            rooms_per_household,\n",
    "            population_per_household,\n",
    "            income_per_household,\n",
    "            ocean_proximity\n",
    "        ]\n",
    "\n",
    "        x_data.append(features)\n",
    "        y_data.append(value)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "x_data = np.array(x_data)\n",
    "y_data = np.array(y_data)\n",
    "\n",
    "# Clip target outliers\n",
    "y_data = np.clip(y_data, a_min=None, a_max=np.percentile(y_data, 99))\n",
    "\n",
    "# Standardize features\n",
    "x_mean = x_data.mean(axis=0)\n",
    "x_std = x_data.std(axis=0)\n",
    "x_data = (x_data - x_mean) / x_std\n",
    "x_data = np.nan_to_num(x_data)\n",
    "\n",
    "# Add bias term\n",
    "x_data = np.hstack([np.ones((x_data.shape[0], 1)), x_data])\n",
    "\n",
    "# Normalize target (min-max)\n",
    "y_min = y_data.min()\n",
    "y_max = y_data.max()\n",
    "y_data = (y_data - y_min) / (y_max - y_min)\n",
    "y_data = y_data.reshape(-1, 1)\n",
    "\n",
    "# Train-test split\n",
    "split = int(0.8 * len(x_data))\n",
    "x_train, x_test = x_data[:split], x_data[split:]\n",
    "y_train, y_test = y_data[:split], y_data[split:]\n",
    "\n",
    "# Gradient Descent with L2 regularization\n",
    "weights = np.random.randn(x_train.shape[1], 1) * 0.01\n",
    "#these value of alpha , epochs gaves result with time taken if we change 0.01->0.001 , 10k ->20k,30k\n",
    "alpha = 0.01\n",
    "epochs = 10000\n",
    "lambda_reg = 0.1\n",
    "n = x_train.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    predictions = x_train @ weights\n",
    "    errors = predictions - y_train\n",
    "    gradient = (x_train.T @ errors) / n + lambda_reg * weights\n",
    "    gradient[0] -= lambda_reg * weights[0]  # no regularization on bias\n",
    "    weights -= alpha * gradient\n",
    "\n",
    "def evaluate(x, y_true, weights):\n",
    "    preds = x @ weights\n",
    "    errors = y_true - preds\n",
    "    mae = np.mean(np.abs(errors))\n",
    "    rmse = np.sqrt(np.mean(errors**2))\n",
    "    y_mean = np.mean(y_true)\n",
    "    total_var = np.sum((y_true - y_mean)**2)\n",
    "    residual_var = np.sum(errors**2)\n",
    "    r2 = 1 - residual_var / total_var\n",
    "    return mae, rmse, r2\n",
    "\n",
    "mae, rmse, r2 = evaluate(x_test, y_test, weights)\n",
    "\n",
    "# Predict custom input example (include ocean_proximity)\n",
    "custom_input = np.array([\n",
    "    -122.23,     # longitude\n",
    "    37.88,       # latitude\n",
    "    41,          # housing_median_age\n",
    "    8.3252,      # median_income\n",
    "    880 / 322,   # rooms_per_person\n",
    "    129 / 880,   # bedrooms_per_room\n",
    "    880 / 126,   # rooms_per_household\n",
    "    322 / 126,   # population_per_household\n",
    "    8.3252 / 126, # income_per_household\n",
    "    259212         # ocean_proximity numeric (example)\n",
    "])\n",
    "\n",
    "custom_input = (custom_input - x_mean) / x_std\n",
    "custom_input = np.nan_to_num(custom_input)\n",
    "custom_input = np.insert(custom_input, 0, 1.0)  # bias term\n",
    "predicted_normalized = custom_input @ weights\n",
    "predicted_normalized = np.clip(predicted_normalized, 0, 1)\n",
    "predicted_value = predicted_normalized * (y_max - y_min) + y_min\n",
    "\n",
    "print(\"Trained weights:\", weights.flatten())\n",
    "print(\"Predicted median_house_value:\", predicted_value.item())\n",
    "print(\"MAE:\", mae * (y_max - y_min))\n",
    "print(\"RMSE:\", rmse * (y_max - y_min))\n",
    "print(\"R² Score:\", r2)\n",
    "#MAE: 48515.653783799724\n",
    "#RMSE: 68558.2030025164\n",
    "#R² Score: 0.6774388584676602"
   ],
   "id": "b58047dba7e38d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [ 0.38914547 -0.02669392 -0.03344972  0.02290499  0.14496131  0.05915814\n",
      "  0.01791219 -0.03939036 -0.0077711  -0.00260295  0.05492466]\n",
      "Predicted median_house_value: 400959.50446354994\n",
      "MAE: 48515.653783799724\n",
      "RMSE: 68558.2030025164\n",
      "R² Score: 0.6774388584676602\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:05:14.045514Z",
     "start_time": "2025-05-19T12:04:46.398255Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8ebcbf8ce55fedf6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [ 0.38888469 -0.01962883 -0.02584124  0.02302891  0.14387209  0.05349689\n",
      "  0.01765066 -0.03373562 -0.00898169 -0.00265787  0.05775227]\n",
      "Predicted median_house_value: 400339.2131064219\n",
      "MAE: 48854.977193454615\n",
      "RMSE: 69021.69236713304\n",
      "R² Score: 0.6730627513501537\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:06:36.901256Z",
     "start_time": "2025-05-19T12:05:56.168555Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "bdb410e9eec162c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained weights: [ 0.38914547 -0.02669397 -0.03344977  0.02290499  0.1449613   0.05915814\n",
      "  0.01791219 -0.03939036 -0.00777109 -0.00260295  0.05492465]\n",
      "Predicted median_house_value: 400959.5027396754\n",
      "MAE: 48515.65224212291\n",
      "RMSE: 68558.20065714387\n",
      "R² Score: 0.6774388805372594\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:09:44.151297Z",
     "start_time": "2025-05-19T12:09:44.146663Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "8dc36474d2d89a48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:09:44.268466Z",
     "start_time": "2025-05-19T12:09:44.266802Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2866fc70f6e5c7ca",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:09:44.397589Z",
     "start_time": "2025-05-19T12:09:44.392514Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f87224ea126349e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-19T12:09:44.586731Z",
     "start_time": "2025-05-19T12:09:44.583501Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "17c024014d135bdc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "cd9a2efcc8307994"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
